[
  {
    "objectID": "index_v1.0.html",
    "href": "index_v1.0.html",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "",
    "text": "AI usage disclosure\n\n\n\n\n\nWe used Claude to help write code and clean up code. We used ChatGPT to help write code, clean up code, write text that we edited, suggest wording ideas and then chose which small phrases or sentence structure ideas to use, expand on summary text that we provided and then edited the text it produced, help clarify and streamline text that we wrote, interpret model training results data, and suggest papers on relevant science, we did further reading, and we cited some of this literature. We also provided ChatGPT with starting text and had it rearrange that text to fit the structure of one of our pub templates. We used Gemini in similar ways to help write code, clean up code, write text that we edited, suggest wording ideas and then chose which small phrases or sentence structure ideas to use, help clarify and streamline text that we wrote, interpret model training results data, and suggest papers on relevant science, we did further reading, and we cited some of this literature. We also provided Gemini with starting text and had it rearrange that text to fit the structure of one of our pub templates, and expanded on summary text that we provided and then edited the text it produced."
  },
  {
    "objectID": "index_v1.0.html#introduction",
    "href": "index_v1.0.html#introduction",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Introduction",
    "text": "Introduction\nOn April 11th, Rijal et al. (2025) released a preprint that introduces an application of attention mechanisms for inferring genotype-phenotype maps, particularly focusing on capturing complex epistatic interactions. This work sparked considerable interest given our ongoing exploration of nonlinear genotype-phenotype models.\nBy training on 100,000 yeast segregants over 18 growth phenotypes, Rijal et al. (2025) demonstrated that an attention-based architecture extracts more epistatic signals than conventional linear methods, and does so with far fewer parameters than the full second-order regression that has become a norm in quantitative genetics. Their study therefore provides an important proof-of-principle for deep-learning architectures being able to cope with linkage disequilibrium, noise, and sparse high-order interactions in large-scale genotype-phenotype data.\nThat success naturally raised our curiosity. The network they used omits several standard transformer components that brought attention into the limelight: skip connections, layer normalisation, and feed-forward sub-layers (Vaswani et al., 2017). Given the cross-domain success that transformers have seen, we wondered: Could the performance be further improved by replacing their model with a standard “vanilla” transformer architecture? We found out. Also, along the way, we uncovered a missed opportunity by Rijal et al. (2025) to reinforce the learning signal by properly leveraging cross-phenotype genetic correlations, which led to significant performance gains.\n\n\n\n\n\n\n Lightning fast shareable research\n\n\n\nWe offer this work not just as a technical add-on but as a proof of how open, fast-moving collaboration can accelerate discovery. Rijal et al. (2025) shared their code and within twenty days of their preprint—and after only nine working days—we were able to validate, extend, and publicly release the next iteration. That cycle is orders of magnitude quicker than the traditional journal pipeline and illustrates the compounding value of doing bio ML in the open. We hope the present study sparks yet another round of improvements."
  },
  {
    "objectID": "index_v1.0.html#the-dataset",
    "href": "index_v1.0.html#the-dataset",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "The dataset",
    "text": "The dataset\nThe experimental data used in Rijal et al. (2025) comes from the work of Nguyen Ba et al. (2022), who performed a large-scale quantitative trait locus (QTL) study in yeast. In short, they measured the growth rates of ~100,000 yeast segregants across 18 conditions and for ~40,000 loci, creating a massive dataset suitable for mapping genotype to phenotype.\nDue to extensive linkage disequilibrium (LD), the loci in the dataset are highly correlated with each other. To create a set of independent loci, Rijal et al. (2025) defined a set of loci such that the correlation between the SNPs present at any pair of loci is less than 94%, resulting in a set of 1164 “independent” loci.\nWe were unable to find the set of loci, nor the genotypic and phenotypic data used for training, so we located the raw data that Nguyen Ba et al. (2022) originally uploaded alongside their study, then used this notebook uploaded by Rijal et al. (2025) to recapitulate the 1164 loci. To save everyone else the trouble, we uploaded the train, test, and validation datasets we’re pretty sure Rijal et al. (2025) used in their study.\nYou can find the data here: https://zenodo.org/records/15313069\nWe’ll use this data in what follows, so let’s go ahead and download it into the current working directory:\n\nimport subprocess\nfrom pathlib import Path\n\ndataset_dir = Path(\"datasets/\")\nif not dataset_dir.exists():\n    zenodo_url = \"https://zenodo.org/records/15313069/files/datasets.zip\"\n    subprocess.run([\"wget\", zenodo_url])\n    subprocess.run([\"unzip\", \"datasets.zip\"])\n    Path(\"datasets.zip\").unlink()\n\n\n\n\n\n\nTable 1. The test dataset R2 values for the\nattention model in Figure 3 of Rijal et al. Calculated by manual pixel\ncounting. {#8d4b415c}\n  \n    \n      \n      Phenotype\n      Source R²\n    \n  \n  \n    \n      0\n      23C\n      0.612\n    \n    \n      1\n      25C\n      0.621\n    \n    \n      2\n      27C\n      0.632\n    \n    \n      3\n      30C\n      0.609\n    \n    \n      4\n      33C\n      0.615\n    \n    \n      5\n      35C\n      0.582\n    \n    \n      6\n      37C\n      0.566\n    \n    \n      7\n      cu\n      0.610\n    \n    \n      8\n      suloc\n      0.638\n    \n    \n      9\n      ynb\n      0.487\n    \n    \n      10\n      eth\n      0.604\n    \n    \n      11\n      gu\n      0.566\n    \n    \n      12\n      li\n      0.630\n    \n    \n      13\n      mann\n      0.600\n    \n    \n      14\n      mol\n      0.618\n    \n    \n      15\n      raff\n      0.653\n    \n    \n      16\n      sds\n      0.630\n    \n    \n      17\n      4NQO\n      0.609"
  },
  {
    "objectID": "index_v1.0.html#reproducing-single-phenotype-results",
    "href": "index_v1.0.html#reproducing-single-phenotype-results",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Reproducing single phenotype results",
    "text": "Reproducing single phenotype results\nLet’s first try and reproduce the single-phenotype attention model performances observed in Figure 3 (red dots). This will give us the confidence to know we both (a) correctly reverse-engineered the specifics of their training/validation/test datasets and (b) accurately implemented their model.\n\n\n\nFigure 3 from Rijal et al. (2025). Original caption: “Comparison of model performance in yeast QTL mapping data. We show R² on test datasets for linear, linear+pairwise, and attention-based model (with d = 12) across 18 phenotypes (relative growth rates in various environments). For the linear + pairwise mode, the causal loci inferred by Nguyen Ba et al. (2022) are used.”\n\n\nNote that a separate model is trained on each phenotype, so reproducing this figure involves training 18 models. To do this, we need to create config objects specifying each model architecture and how the training should proceed.\n\n\nCode\nimport attrs\nfrom analysis.base import ModelConfig, TrainConfig\nfrom analysis.dataset import phenotype_names\n\nmodel_config = ModelConfig(\n    model_type=\"rijal_et_al\",\n    seq_length=1164,\n    embedding_dim=13,\n    num_layers=3,\n)\n\n# This template config sets all the shared parameters.\ntrain_template_config = TrainConfig(\n    data_dir=dataset_dir,\n    save_dir=Path(\"models/fig3\"),\n    name_prefix=\"fig3_23C\",\n    phenotypes=[\"23C\"],\n    optimizer=\"adam\",\n    batch_size=64,\n    learning_rate=0.001,\n    lr_schedule=False,\n    weight_decay=0.0,\n    max_epochs=200,\n    gradient_clip_val=0,\n    use_cache=True,\n    use_modal=False,\n)\n\njobs = []\nfor phenotype in phenotype_names:\n    # Each train config needs to set its corresponding phenotype(s).\n    phenotype_config = attrs.evolve(\n        train_template_config, phenotypes=[phenotype], name_prefix=f\"fig3_{phenotype}\"\n    )\n    jobs.append((model_config, phenotype_config))\n\n\nWith each job properly configured, we can train a model for each phenotype:\n\n\nCode\nfrom analysis.train import run_trainings\n\nmodel_dirs = run_trainings(jobs)\n\n\nPre-trained model 'fig3_23C' found. Returning path.\nPre-trained model 'fig3_25C' found. Returning path.\nPre-trained model 'fig3_27C' found. Returning path.\nPre-trained model 'fig3_30C' found. Returning path.\nPre-trained model 'fig3_33C' found. Returning path.\nPre-trained model 'fig3_35C' found. Returning path.\nPre-trained model 'fig3_37C' found. Returning path.\nPre-trained model 'fig3_cu' found. Returning path.\nPre-trained model 'fig3_suloc' found. Returning path.\nPre-trained model 'fig3_ynb' found. Returning path.\nPre-trained model 'fig3_eth' found. Returning path.\nPre-trained model 'fig3_gu' found. Returning path.\nPre-trained model 'fig3_li' found. Returning path.\nPre-trained model 'fig3_mann' found. Returning path.\nPre-trained model 'fig3_mol' found. Returning path.\nPre-trained model 'fig3_raff' found. Returning path.\nPre-trained model 'fig3_sds' found. Returning path.\nPre-trained model 'fig3_4NQO' found. Returning path.\n\n\n\n\n\n\n\n\nA note about training behavior\n\n\n\nThe above code will initiate training for all configured jobs, with important considerations:\n\nCaching: Since train_config.use_cache = True and these models are stored in the GitHub repository, executing this locally will use cached models instead of performing expensive retraining\nTraining Duration: Each training job takes approximately 2.5 hours to complete on an A10G GPU, so running all jobs without caching would require significant time\nCompute Configuration: In our production environment, we set train_config.use_modal = True to distribute compute jobs via Modal. For compatibility with different compute architectures, this notebook uses train_config.use_modal = False by default.\n\n\n\nInside each run directory is a metrics.csv that we can get the test dataset \\(R^2\\) from and compare directly against Figure 3 from Rijal et al. (2025).\n\n\nCode\nfrom pathlib import Path\n\nimport arcadia_pycolor as apc\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\napc.mpl.setup()\n\n\ndef get_test_r2(model_dir: Path) -&gt; float:\n    metrics = pd.read_csv(model_dir / \"metrics.csv\")\n    return float(metrics.loc[metrics[\"metric\"] == \"test_r2\", \"value\"].iloc[0])\n\n\nfig3_reproduction_r2 = [get_test_r2(d) for d in model_dirs]\nfig3_results[\"reproduction R2\"] = fig3_reproduction_r2\ndf = pd.DataFrame(fig3_results)\n\nx = np.arange(len(df))\nwidth = 0.35\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    plt.bar(x - width / 2, df[\"source R2\"], width, label=\"Source R²\", color=apc.cloud)\n    plt.bar(x + width / 2, df[\"reproduction R2\"], width, label=\"Reproduction R²\", color=apc.steel)\n\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"R² Score\")\n    plt.ylim(0.46, 0.67)\n    plt.xticks(x, df[\"phenotype\"], rotation=90)\n    plt.legend(loc=(0.05, 0.90), ncol=2)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\nFigure 1: Comparison of test-set \\(R^2\\) values between Rijal et al. (2025) (“Source”) and our re-implementation (“Reproduction”).\n\n\n\n\n\nWith an \\(n\\) of 1, we can’t assess reproducibility rigorously. Even so, our re-implementation matches the published numbers very closely. Here are some numbers on the phenotype with the largest deviation:\n\n\nCode\ndeltas = (df[\"source R2\"] - df[\"reproduction R2\"]).abs()\nmax_idx = deltas.argmax()\nmax_delta = deltas.max()\nphenotype_with_largest_delta = df[\"phenotype\"].iloc[max_idx]\npercent_diff = 100 * max_delta / df[\"source R2\"].iloc[max_idx]\n\nprint(\"Biggest discrepenacy:\")\nprint(f\"- Phenotype: {phenotype_with_largest_delta}\")\nprint(f\"- R2 difference: {max_delta:.3f}\")\nprint(f\"- Percent difference: {percent_diff:.1f}\")\n\n\nBiggest discrepenacy:\n- Phenotype: 33C\n- R2 difference: 0.008\n- Percent difference: 1.3\n\n\nOverall, these results help assure us that:\n\nThe train/validation/test partitions are identical, or at least functionally equivalent\nOur code reproduces the authors’ architecture and training procedure.\n\nWith this validated baseline in place, we’re now confident in using it as the starting point for modifying the architecture."
  },
  {
    "objectID": "index_v1.0.html#canonical-ml-components-outperform-bespoke-customization",
    "href": "index_v1.0.html#canonical-ml-components-outperform-bespoke-customization",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Canonical ML components outperform bespoke customization",
    "text": "Canonical ML components outperform bespoke customization\nThe Rijal et al. (2025) model architecture uses non-standard components. To test whether these idiosyncratic choices are actually helpful, we replaced each one with a standard, “textbook” alternative and measured the collective impact on predictive accuracy.\n\n\n\n\n\n\n\n\nNon-standard element in Rijal et al.\nWhat it does\nCanonical replacement we tried\n\n\n\n\nRandom projection of a diagonal genotype matrix X(g)·R\nEncodes each locus as a row of a fixed random matrix R, multiplied by the allele sign (±1).\nA learned embedding table nn.Embedding(L, D) whose rows are multiplied by the allele sign.\n\n\nConcatenated column of ones\nAppends a constant 1 to every token to mimic an explicit bias term (Appendix F in the original paper).\nSimply enable the bias in the linear Q, K, V projections (nn.Linear(..., bias=True)).\n\n\nPhenotype represented as an input token\nAdds one-hot phenotype tokens to the sequence, forcing attention layers to discover gene × phenotype interactions.\nCondition the model after the attention block: predict all phenotypes jointly from a pooled sequence representation (see below).\n\n\nFlattened fitness matrix\nTreats each (genotype, phenotype) pair as an independent sample; the network outputs one scalar at a time.\nMean-pool the token embeddings → (B, D) and use a single linear layer D → 18 so all phenotypes are predicted simultaneously.\n\n\n\n\nWhy predict all phenotypes at once?\nThe measured phenotypes are likely to be genetically correlated due to biological processes such as pleiotropy. Consequently, knowing that a mutation hurts growth in one condition has the potential to inform its effect in another. A shared output head lets the network exploit this mutual information, whereas the original set-up can only share knowledge through the shared attention weights. Our phenotype-phenotype autoencoder study (Avasthi et al., 2023) showed significant prediction benefits when accounting for phenotypic covariation, so we’re expecting the same thing to be true here, too.\n\n\nExperimental protocol\nThe codebase contains a new architecture with the above modifications. Let’s test its performance with the following changes to the training:\n\nIncrease the number of phenotypes from \\(1\\) to \\(18\\) (all).\nCorrespondingly increase the hidden dimension from \\(d=12\\) to \\(d=128\\).\nDecrease the learning rate 10-fold, from \\(1 \\times 10^{-3}\\) to \\(1 \\times 10^{-4}\\).\n\nFinally, because Figure 1 hints at potentially significant run-to-run variance, let’s run five replicates, where each replicate differs only in its initializing seed (the train/validation/test splits are held constant).\n\n\nCode\nNUM_REPLICATES = 5\n\njobs = []\nfor i in range(NUM_REPLICATES):\n    replicate_id = f\"{i:02d}\"\n    job_name_prefix = f\"std_d128_rep_{replicate_id}\"\n\n    model_config = ModelConfig(\n        embedding_dim=128,\n        model_type=\"modified\",\n        seq_length=1164,\n        num_layers=3,\n    )\n\n    train_config = attrs.evolve(\n        train_template_config,\n        save_dir=Path(\"models/canonical\"),\n        phenotypes=phenotype_names,\n        name_prefix=job_name_prefix,\n        optimizer=\"adamw\",\n        max_epochs=100,\n    )\n\n    jobs.append((model_config, train_config))\n\nprint(f\"\\nGenerated {len(jobs)} job configurations.\")\n\n\n\nGenerated 5 job configurations.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re running this yourself and aren’t interested in replicates, you can reduce the amount of required compute by setting NUM_REPLICATES in the above cell to 1.\n\n\nNow, let’s run the experiment:\n\n\nCode\nmodel_dirs = run_trainings(jobs)\n\n\nPre-trained model 'std_d128_rep_00' found. Returning path.\nPre-trained model 'std_d128_rep_01' found. Returning path.\nPre-trained model 'std_d128_rep_02' found. Returning path.\nPre-trained model 'std_d128_rep_03' found. Returning path.\nPre-trained model 'std_d128_rep_04' found. Returning path."
  },
  {
    "objectID": "index_v1.0.html#consistent-gains-across-phenotypes",
    "href": "index_v1.0.html#consistent-gains-across-phenotypes",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Consistent gains across phenotypes",
    "text": "Consistent gains across phenotypes\nFigure 2 shows that swapping the custom choices in Rijal et al. (2025) for a canonical embedding \\(\\rightarrow\\) mean-pool \\(\\rightarrow\\) linear-head model and predicting all 18 phenotypes jointly yields a boost in predictive power across the board:\n\n\nCode\nlabel_lookup = {\n    \"std\": \"Canonical\",\n}\n\n\ndef get_phenotype_r2_data(model_dir: Path) -&gt; float:\n    # Determine the architecture and replicate number from model directory name\n    dir_name = model_dir.parent.parent.name  # e.g., \"std_d128_rep_09\"\n    variant_key, _, rep_str = dir_name.rpartition(\"_rep_\")\n    base_arch_key, _, _ = variant_key.rpartition(\"_d\")\n    architecture_label = label_lookup[base_arch_key]\n\n    # Load and wrangle the metrics.csv. Keep only the per-phenotype test R2 values.\n    df = pd.read_csv(model_dir / \"metrics.csv\")\n    df = df[df[\"metric\"].str.startswith(\"test_r2_\")]\n    df[\"phenotype\"] = df[\"metric\"].str[8:]\n    df.drop(\"metric\", axis=1, inplace=True)\n    df.rename(columns={\"value\": \"r2\"}, inplace=True)\n    df[\"architecture\"] = architecture_label\n    df[\"replicate\"] = int(rep_str)\n    df[\"r2\"] = df[\"r2\"].astype(float)\n\n    return df\n\n\n# Concat the phenotype data across all models\ncanonical_plot_data = pd.concat([get_phenotype_r2_data(model_dir) for model_dir in model_dirs])\n\n# Calculate the mean and standard error R2 over replicates\ncanonical_plot_data = (\n    canonical_plot_data.groupby([\"architecture\", \"phenotype\"])[\"r2\"]\n    .agg(mean_r2=\"mean\", std_r2=\"std\")\n    .reset_index()\n)\n\n# Concatenate the manually scraped Fig3 R2s\nrijal_r2s = fig3_results.copy(deep=True)\nrijal_r2s.drop(\"reproduction R2\", axis=1, inplace=True)\nrijal_r2s.rename(columns={\"source R2\": \"mean_r2\"}, inplace=True)\nrijal_r2s[\"std_r2\"] = 0.0\nrijal_r2s[\"architecture\"] = \"Rijal\"\ncanonical_plot_data = pd.concat([canonical_plot_data, rijal_r2s])\n\n# Plotting code.\ncanonical_plot_data[\"phenotype\"] = pd.Categorical(\n    canonical_plot_data[\"phenotype\"], categories=phenotype_names, ordered=True\n)\narch_order = [\"Rijal\", \"Canonical\"]\n\nx = np.arange(len(phenotype_names))\nwidth = 0.25\n\ncolor_map = {\n    \"Rijal\": apc.cloud,\n    \"Canonical\": apc.mars,\n}\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    for i, arch in enumerate(arch_order):\n        sub = (\n            canonical_plot_data[canonical_plot_data[\"architecture\"] == arch]\n            .set_index(\"phenotype\")\n            .reindex(phenotype_names)\n        )\n        plt.bar(\n            x + (i - 0.5) * width,  # centre bars around tick\n            sub[\"mean_r2\"],\n            width,\n            yerr=sub[\"std_r2\"],\n            ecolor=\"#00000099\",\n            error_kw=dict(elinewidth=1.5),\n            label=arch,\n            color=color_map[arch],\n        )\n\n    plt.xticks(x, phenotype_names, rotation=90)\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"Mean $R^2$ (± SD)\")\n    plt.ylim(0.43, 0.69)\n    plt.legend(loc=(0.05, 0.92), ncol=3)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\nFigure 2: Per-phenotype generalisation of multi-task vs. single-task models. Bar heights show the mean test-set \\(R^{2}\\) achieved across replicates for each phenotype, with error bars denoting the standard deviation (SD). “Rijal” is the single-phenotype results reported in Rijal et al. (2025), while “Canonical” is our multi-output head trained to predict all phenotypes jointly.\n\n\n\n\n\n\n\n\n\n\n\nImportant caveat on the baseline we compare against\n\n\n\nThe numbers labeled “Rijal” in Figure 2 come from the single-phenotype models reported in their Figure 3. Rijal et al. (2025) do develop a multi-phenotype architecture, but careful inspection of their code shows that it still emits a scalar output per forward pass and simply loops over phenotypes at train time. In other words, each (genotype, phenotype) pair is treated as an independent sample. The predictive power of this model is shown in their Figure 4, and is actually worse than the single-phenotype models at predicting any given phenotype.\nBecause our goal is to test whether a genuine multi-output head + canonical components confer an advantage, we chose the best-performing baseline available (their single-phenotype models). The comparison is therefore conservative: any gains we report would be larger, not smaller, if the authors’ scalar multi-phenotype model were used as the reference.\n\n\nQuantifying which changes contributed to these gains would require an ablation study, however our hunch is that the primary gains come from cross-phenotype genetic correlations. For instance we note that some of the most consistent gains we see in our implementation of the model come from fitness measurements at different temperatures, a set of phenotypes that is almost certainly impacted by pleiotropic sets of genes. Joint training allows the network to transfer information among genetically correlated traits, an advantage that the single-output baseline can’t exploit beyond implicit relationships learned through shared attention weights.\nTaken together, these results validate the canonical multi-output model as a stronger starting point than the original design."
  },
  {
    "objectID": "index_v1.0.html#a-vanilla-transformer-architecture",
    "href": "index_v1.0.html#a-vanilla-transformer-architecture",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "A vanilla transformer architecture",
    "text": "A vanilla transformer architecture\nThe current architecture still departs from the reference transformer in several respects: it omits residual connections, layer normalization, and position-wise feed-forward blocks. The logical next experiment is therefore to level up to a bona-fide vanilla transformer, preserving the current tokenization while adding:\n\nResidual (skip) connections and pre-LayerNorm,\n\nFeed-forward sub-layers with RELU activations,\n\nScaled dot-product attention with the canonical \\(1/\\sqrt{d}\\) factor,\n\nDropout and weight-decay for regularization to prevent overfitting.\n\nIt’s worth noting that transformers excel at sequence modeling—and sequences are ordered collections. While the loci in this dataset could be attributed chromosomal coordinates, and therefore in a sense an ordering, like Rijal et al. (2025), we’re treating the loci as an unordered collection. Thus, we don’t add any positional encodings, either in the form of absolute or relative positional encodings.\n\n\nCode\nNUM_REPLICATES = 5\n\njobs = []\nfor i in range(NUM_REPLICATES):\n    replicate_id = f\"{i:02d}\"\n    job_name_prefix = f\"xformer_rep_{replicate_id}\"\n\n    model_config = ModelConfig(\n        embedding_dim=256,\n        model_type=\"transformer\",\n        seq_length=1164,\n        num_layers=3,\n        nhead=4,\n        dim_feedforward=1024,\n    )\n\n    train_config = attrs.evolve(\n        train_template_config,\n        save_dir=Path(\"models/transformer\"),\n        phenotypes=phenotype_names,\n        name_prefix=job_name_prefix,\n        optimizer=\"adamw\",\n        max_epochs=80,\n    )\n\n    jobs.append((model_config, train_config))\n\nprint(f\"\\nGenerated {len(jobs)} job configurations.\")\n\n\n\nGenerated 5 job configurations.\n\n\n\n\nCode\nmodel_dirs = run_trainings(jobs)\n\n\nPre-trained model 'xformer_rep_00' found. Returning path.\nPre-trained model 'xformer_rep_01' found. Returning path.\nPre-trained model 'xformer_rep_02' found. Returning path.\nPre-trained model 'xformer_rep_03' found. Returning path.\nPre-trained model 'xformer_rep_04' found. Returning path.\n\n\nFigure 3 shows the change in performance:\n\n\nCode\nlabel_lookup = {\n    \"std\": \"Canonical\",\n    \"xformer\": \"Transformer\",\n}\n\n\ndef get_phenotype_r2_data_xformer(model_dir: Path) -&gt; float:\n    # Determine the architecture and replicate number from model directory name\n    dir_name = model_dir.parent.parent.name  # e.g., \"std_d128_rep_09\"\n    variant_key, _, rep_str = dir_name.rpartition(\"_rep_\")\n    architecture_label = label_lookup[variant_key]\n\n    # Load and wrangle the metrics.csv. Keep only the per-phenotype test R2 values.\n    df = pd.read_csv(model_dir / \"metrics.csv\")\n    df = df[df[\"metric\"].str.startswith(\"test_r2_\")]\n    df[\"phenotype\"] = df[\"metric\"].str[8:]\n    df.drop(\"metric\", axis=1, inplace=True)\n    df.rename(columns={\"value\": \"r2\"}, inplace=True)\n    df[\"architecture\"] = architecture_label\n    df[\"replicate\"] = int(rep_str)\n    df[\"r2\"] = df[\"r2\"].astype(float)\n\n    return df\n\n\n# Concat the phenotype data across all models\ndf = pd.concat([get_phenotype_r2_data_xformer(model_dir) for model_dir in model_dirs])\ndf = df.groupby([\"architecture\", \"phenotype\"])[\"r2\"].agg(mean_r2=\"mean\", std_r2=\"std\").reset_index()\n\n# Concat with existing canonical plot data\nxformer_plot_data = pd.concat([df, canonical_plot_data])\n\n# Plotting code.\nxformer_plot_data[\"phenotype\"] = pd.Categorical(\n    xformer_plot_data[\"phenotype\"], categories=phenotype_names, ordered=True\n)\narch_order = [\"Rijal\", \"Canonical\", \"Transformer\"]\n\nx = np.arange(len(phenotype_names))\nwidth = 0.25\n\ncolor_map = {\n    \"Rijal\": apc.cloud,\n    \"Canonical\": apc.mars,\n    \"Transformer\": apc.dragon,\n}\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    for i, arch in enumerate(arch_order):\n        sub = (\n            xformer_plot_data[xformer_plot_data[\"architecture\"] == arch]\n            .set_index(\"phenotype\")\n            .reindex(phenotype_names)\n        )\n        plt.bar(\n            x + (i - 1) * width,  # centre bars around tick\n            sub[\"mean_r2\"],\n            width,\n            yerr=sub[\"std_r2\"],\n            ecolor=\"#00000099\",\n            error_kw=dict(elinewidth=1.5),\n            label=arch,\n            color=color_map[arch],\n        )\n\n    plt.xticks(x, phenotype_names, rotation=90)\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"Mean $R^2$ (± SD)\")\n    plt.ylim(0.43, 0.69)\n    plt.legend(loc=(0.05, 0.92), ncol=3)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\nFigure 3: The vanilla transformer performances added alongside the models compared in Figure 2.\n\n\n\n\n\nThe full transformer nudges performance above the already-improved canonical model across most phenotypes (Figure 3). The lift is apparent, but no stepwise performance gains are observed. There’s undoubtedly a lot of room for improvement: we made no attempt at hyper-parameter tuning beyond some cursory investigations of dropout and weight-decay (neither of which positively affected performance), and all other parameters were guessed zero shot."
  },
  {
    "objectID": "index_v1.0.html#conclusion-and-outlook",
    "href": "index_v1.0.html#conclusion-and-outlook",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Conclusion and outlook",
    "text": "Conclusion and outlook\nOur central question was simple: do off-the-shelf transformer components—and a genuine multi-output head—move the needle on genotype to phenotype prediction?\nThe answer is an emphatic yes.\n\nWhy multi-output matters\nLeveraging mutual information between genetically correlated phenotypes represents a natural way of boosting model performance when training data is limited. In the context of plant/animal breeding, multi-trait genomic prediction has been well established to improve the predictive power of linear models (particularly for low heritability traits) (Jia and Jannink, 2012). Our own work with phenotype-phenotype autoencoders has demonstrated that encoding multiple phenotypes jointly lets an auto-encoder predict individual phenotypes with very high accuracy, particularly as the number of phenotypes considered simultaneously increases (Avasthi et al., 2023). These observations are examples of the benefits of multi-task learning, which have long been appreciated in ML literature (Caruana, 1997). Our presented results are no different: when the network can see all 18 phenotypes at once, shared genetic effects (either through pleiotropy or in some instances through LD (Chebib and Guillaume, 2021)) reinforce, rather than fragment, the learning signal.\n\n\nA foundation, not a finish line\nA natural next step given these results is to test cross-environment transfer learning in our multi-task model. Rijal et al. (2025) demonstrated that transfer learning was possible across temperature growth conditions even with a multi-environment model that did not outperform a single-environment model. Given the overall superior performance of the models we present here, they may be particularly well suited for fitness prediction for novel phenotypes with even fewer fine-tuning observations than used by Rijal et al. (2025).\nFinally, it would be interesting to evaluate where the performance boost of multi-task learning comes from, from a quantitative genetics perspective. Phenotypic variance can generally be broken down into additive (\\(G\\)), epistatic (\\(G \\times G\\)) and gene by environment (\\(G \\times E\\)) components (ignoring higher order terms for simplicity). Intuitively, we might expect multi-trait learning to excel at explaining \\(G\\) and \\(G \\times G\\) variance components that are constant across environments, however it’s possible that \\(G \\times E\\) variance is also better explained, as multi-task learning benefits are known to extend to even unrelated tasks (Paredes et al., 2012). We suspect this could be investigated through ablation studies that track environment specific prediction outcomes.\nThere is also still plenty of headroom for improving the model:\n\nNo hyper-parameter search was attempted for the transformer; depth, head count, LR schedules, dropout rates, and layer widths all remain untouched.\nHow far can the canonical model be pushed? Maybe the transformer is overkill.\nThe model treats the loci as an unordered collection. It’s possible that adding chromosomal coordinates or some kind of other positional encoding could let the model sing.\nWe haven’t experimented with more sophisticated feature selection methods for reducing the number of loci prior to model training, a task that may be particularly fruitful for improving the ability of models to capture pairwise and higher order epistasis.\n\n\n\nAn invitation to build further\nAll code, configs, and cached model checkpoints are available in this notebook’s repository. The Appendix documents the engineering decisions that should help orient you to the codebase."
  },
  {
    "objectID": "index_v1.0.html#appendix-a-codebase-primer",
    "href": "index_v1.0.html#appendix-a-codebase-primer",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Appendix: a codebase primer",
    "text": "Appendix: a codebase primer\n\n\n\n\n\n\nNote\n\n\n\nThis section elaborates on engineering details that will be of interest to those planning to reproduce, modify, or build off of this research.\n\n\nRather than building off the Rijal et al. (2025) notebook files, we re-implemented their code into our own codebase to improve code quality and make room for our modifications and experimentation. Here is a very high-level summary of the changes we made:\n\nAdded configuration dataclasses to co-localize tunables\nSaved the training/validation/test datasets to file to avoid splitting on-the-fly\nCreated PyTorch dataloaders to manage accession, batching, and data shuffling\nAutomated the training loop with PyTorch Lightning, creating separation between the training loop and the model logic\nAdded canonical learning parameters like early stopping, learning rate scheduling, gradient norm clipping, weight decay, and more\nGeneralized the Rijal et al. (2025) model with toggleable skip connections, layer normalization, scaled attention, dropout rate, and more\n\nThe upshot is that we’re proud of this code and think it establishes a much-needed foundation that can be used to build off the research seeded by Rijal et al. (2025).\n\nTraining models\nIn the analysis above, we illustrated how multiple training jobs can be run using the high level entrypoint, run_trainings, that:\n\nTrains a model for a given set of phenotypes\nDetermines the best model, defined as the model with the lowest loss (MSE) calculated over the validation dataset\nReports the \\(R^2\\) for the test dataset using the best model\nSaves the model to file for downstream use\n\nThe codebase also exposes equivalent behavior through a command-line interface (CLI). It can be accessed via:\n$ python -m analysis.train --help\n\n\nDistributed computing\nWe performed the analysis using Modal’s cloud infrastructure to distribute computations across GPUs, allowing us to rapidly measure performance across many different model architectures and training specifications. Whether you want to train with or without Modal can be toggled by the attribute train.use_modal. By default, Modal execution is disabled. The downside is that your training jobs will run in serial, rather than being distributed across different machines.\n\n\nCaching behavior\nWe implemented a simple cache mechanism that avoids training if a model directory for a given training config already exists. We did this so that GPUs aren’t a requirement for engaging with this research.\n\ntrain_config.use_cache = True (default): Skips retraining if a model with the same configuration already exists\ntrain_config.use_cache = False: Forces retraining regardless of existing models\n\n\n\n\n\n\n\nNote\n\n\n\nAll training runs in this analysis use the default caching mode (train_config.use_cache = True), and the results are git tracked. As a result, if you execute this notebook locally, these models will be loaded from cache rather than retrained."
  },
  {
    "objectID": "examples/demo.html",
    "href": "examples/demo.html",
    "title": "A brief syntax demo",
    "section": "",
    "text": "This notebook demos some key features. For a more extensive resource, see Quarto’s excellent documentation."
  },
  {
    "objectID": "examples/demo.html#introduction",
    "href": "examples/demo.html#introduction",
    "title": "A brief syntax demo",
    "section": "",
    "text": "This notebook demos some key features. For a more extensive resource, see Quarto’s excellent documentation."
  },
  {
    "objectID": "examples/demo.html#text",
    "href": "examples/demo.html#text",
    "title": "A brief syntax demo",
    "section": "Text",
    "text": "Text\n\nHeaders\nh1 headers (# &lt;HEADER-TEXT&gt;) are reserved for the title of the pub, so use h2 (## &lt;HEADER-TEXT&gt;) for section titles and h3, h4, etc. for sub-sections.\n\n\nCallouts\nTo draw more attention to a piece of text, use callouts:\n\n\n\n\n\n\nImportant\n\n\n\nThe most effective way to see the rendered pub is to setup a live preview that re-renders the pub whenever you save this file. Do that with make preview.\n\n\n\n\nCitations & Footnotes\nTo cite something, add its bibtex entry to ref.bib and then cite it (Avasthi et al., 2024). Here’s another (Lin et al., 2023). For in-depth description of available citation syntax, visit Quarto’s documentation.\nAlso, don’t forget about footnotes1.\n1 To add additional information, like what you’re reading right now, use footnotes.\nTo create a multi-paragraph footnote, indent subsequent paragraphs. Footnotes can also cite things (Avasthi et al., 2024)."
  },
  {
    "objectID": "examples/demo.html#math",
    "href": "examples/demo.html#math",
    "title": "A brief syntax demo",
    "section": "Math",
    "text": "Math\nRender math2 using standard \\(\\LaTeX\\) syntax. Inline with $...$ and display with $$...$$.\n2 Quarto uses MathJax for math rendering.\\[\ne^{ \\pm i\\theta } = \\cos \\theta \\pm i\\sin \\theta\n\\tag{1}\\]\nEuler’s equation (Equation 1) is pretty."
  },
  {
    "objectID": "examples/demo.html#code",
    "href": "examples/demo.html#code",
    "title": "A brief syntax demo",
    "section": "Code",
    "text": "Code\nWrite code as you would in any Jupyter notebook.\n\ndef alertness(hours_sleep, coffees):\n    base_alertness = min(hours_sleep / 8 * 100, 100)\n    coffee_boost = min(coffees * 30, 60)\n    total = min(base_alertness + coffee_boost, 100)\n    return round(total, 1)\n\n\nprint(\"Alertness stats:\")\nprint(f\"4hrs sleep + 1 coffee: {alertness(4, 1)}%\")\nprint(f\"8hrs sleep + 0 coffee: {alertness(8, 0)}%\")\nprint(f\"2hrs sleep + 3 coffee: {alertness(2, 3)}%\")\n\nAlertness stats:\n4hrs sleep + 1 coffee: 70.0%\n8hrs sleep + 0 coffee: 100.0%\n2hrs sleep + 3 coffee: 85.0%\n\n\n\nVisibility & Placement\nSpecify per-block instructions with comments at the top of the code block.\nFold the code block (#| code-fold: true):\n\n\nSource code for this table\nimport pandas as pd\n\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [True, True, False], \"c\": [\"marco\", \"polo\", \"marco\"]})\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      marco\n    \n    \n      1\n      2\n      True\n      polo\n    \n    \n      2\n      3\n      False\n      marco\n    \n  \n\n\n\n\nSuppress code block visibility while retaining the cell output (#| echo: false):\n\n\n\n\n\n\nNote\n\n\n\nThe code block below runs and the output is visible, but the code itself is absent from the rendering.\n\n\n\n\nThe code that generated this print statement is hidden.\n\n\nRender the output in different places, like in the right margin (#| column: margin):\n\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      marco\n    \n    \n      1\n      2\n      True\n      polo\n    \n    \n      2\n      3\n      False\n      marco\n    \n  \n\n\n\nIn general, content placement in highly customizable. For more options, see this Quarto resource.\n\n\nAnnotation\nYou can annotate lines of code. Lines will reveal their annotation when the user hovers over the circled number on the right hand side of the code block.\n\ndef alertness(hours_sleep, coffees):\n1    base_alertness = min(hours_sleep / 8 * 100, 100)\n2    coffee_boost = min(coffees * 20, 60)\n3    total = min(base_alertness + coffee_boost, 100)\n    return round(total, 1)\n\n\n1\n\nScale to percentage, cap at 100\n\n2\n\nEach coffee adds 20%, max 60% boost\n\n3\n\nCap total at 100%\n\n\n\n\nFor details, see Quarto’s code annotation documentation.\n\n\nCodebase\nYou can choose to either fold (#| code-fold: true) or supress (#| echo: false) code snippets that distract from the narrative. However, if you’ve written an extensive amount of code, it may be more practical to define it in a package that this notebook imports from, rather than defining it in the notebook itself. This project is already set up to import from packages found in the src/ directory, so place any such code there. As an example, this code block imports code from a placeholder analysis package found at src/analysis.\n\n1from analysis import polo_if_marco\n\npolo_if_marco(\"marco\")\n\n\n1\n\nSource code\n\n\n\n\n'polo'\n\n\nIf you want to package any of your code for the purposes of simplifying this notebook, replace the contents of src/analysis/ with your own package."
  },
  {
    "objectID": "examples/demo.html#figures-tables",
    "href": "examples/demo.html#figures-tables",
    "title": "A brief syntax demo",
    "section": "Figures & Tables",
    "text": "Figures & Tables\n\nCaptions & Labeling\nIn general, if a cell output is a figure or table, you should caption and label it.\n\ndf\n\n\n\nTable 1: This is a small table.\n\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      marco\n    \n    \n      1\n      2\n      True\n      polo\n    \n    \n      2\n      3\n      False\n      marco\n    \n  \n\n\n\n\n\n\n\nThis is how you reference Table 1.\n\n\n\n\n\n\nNote\n\n\n\nIf the cell output is a table, the label ID should be prefixed with tbl-. If it’s a figure, prefix with fig-.\nFor example, a table could be captioned and labeled with:\n#| label: tbl-small-table\n#| tbl-cap: \"This is a small table.\"\nAnd a figure could be captioned and labeled with:\n#| label: fig-some-figure\n#| fig-cap: \"This is some figure.\"\n\n\nIf your code block produces several plots, you can subcaption each:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef random_plot():\n    plt.figure()\n    plt.scatter(np.random.rand(10), np.random.rand(10), marker=\"o\")\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\nfor _ in range(4):\n    random_plot()\n\n\n\n\n\n\n\n\n\n\n\n(a) This is the first plot.\n\n\n\n\n\n\n\n\n\n\n\n(b) This is the second.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The third.\n\n\n\n\n\n\n\n\n\n\n\n(d) And finally, here’s the fourth.\n\n\n\n\n\n\n\nFigure 1: A panel of scatter plots.\n\n\n\nFigure 1 is just one example layout for multi-panel figures. For more customization options, see Quarto’s documentation on figures.\n\n\nInteractivity\nInteractive widgets can be used. For example, Plotly:\n\nimport plotly.express as px\n\ndf = px.data.gapminder()\npx.scatter(\n    df,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    animation_frame=\"year\",\n    animation_group=\"country\",\n    size=\"pop\",\n    color=\"continent\",\n    hover_name=\"country\",\n    log_x=True,\n    size_max=55,\n    range_x=[100, 100000],\n    range_y=[25, 90],\n)\n\n                                                \n\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s possible that your local preview fails to render the above widget, and you instead see something to the effect of:\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\nIf you want to see how your widget renders within the pub, run make execute and then start a new preview (make preview)."
  },
  {
    "objectID": "pages/FAQ.html",
    "href": "pages/FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "This notebook publication uses a format we’re experimenting with that treats a scientist’s working computational analysis as the publication itself, dissolving the separation that exists between code and publication. Our hypothesis is that this lets us publish faster, promote early-stage work, and increase reproducibility. For details, see our commentary on notebook publications.\n\n\n\nThere is a comment section at the bottom of the pub, where you can read and contribute to any community discussion. Note that commenting requires a GitHub account and authorizing Giscus, a GitHub Discussions widget.\n\n\n\nAll the code for this publication and its analysis are hosted on GitHub at this URL. Any associated data is either hosted or linked to from this repository.\n\n\n\nReproducing this publication is as easy as issuing a few commands from the command line. Follow this setup guide to get started.\n\n\n\nWe welcome improvements to this publication! Please see our guide for contributing."
  },
  {
    "objectID": "pages/FAQ.html#what-is-this",
    "href": "pages/FAQ.html#what-is-this",
    "title": "FAQ",
    "section": "",
    "text": "This notebook publication uses a format we’re experimenting with that treats a scientist’s working computational analysis as the publication itself, dissolving the separation that exists between code and publication. Our hypothesis is that this lets us publish faster, promote early-stage work, and increase reproducibility. For details, see our commentary on notebook publications."
  },
  {
    "objectID": "pages/FAQ.html#how-can-i-comment",
    "href": "pages/FAQ.html#how-can-i-comment",
    "title": "FAQ",
    "section": "",
    "text": "There is a comment section at the bottom of the pub, where you can read and contribute to any community discussion. Note that commenting requires a GitHub account and authorizing Giscus, a GitHub Discussions widget."
  },
  {
    "objectID": "pages/FAQ.html#where-is-the-datacode",
    "href": "pages/FAQ.html#where-is-the-datacode",
    "title": "FAQ",
    "section": "",
    "text": "All the code for this publication and its analysis are hosted on GitHub at this URL. Any associated data is either hosted or linked to from this repository."
  },
  {
    "objectID": "pages/FAQ.html#how-can-i-reproduce-this",
    "href": "pages/FAQ.html#how-can-i-reproduce-this",
    "title": "FAQ",
    "section": "",
    "text": "Reproducing this publication is as easy as issuing a few commands from the command line. Follow this setup guide to get started."
  },
  {
    "objectID": "pages/FAQ.html#how-can-i-contribute",
    "href": "pages/FAQ.html#how-can-i-contribute",
    "title": "FAQ",
    "section": "",
    "text": "We welcome improvements to this publication! Please see our guide for contributing."
  },
  {
    "objectID": "pages/CONTRIBUTING.html",
    "href": "pages/CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "We welcome improvements to this publication! If you’d like to improve or extend the publication, please submit a pull request. We’ll collaborate with you to incorporate your revisions. Alternatively, you’re welcome to leave a comment on the pub using Giscus.\n\nDid you spot any mistakes?\nDo you think an analysis is missing?\nDo you think the wording could be improved?\nDid you spot a typo or grammatical mistake?\n\nThese are just a few examples of revisions that we’d be happy to receive from you.\n\n\n\n\n\n\nNote\n\n\n\nTo learn about how we credit external collaborators, click here.\n\n\n\n\nIf you haven’t already, follow our setup guide to create a local copy of the code and compute environment.\n\n\n\nEdit index.ipynb to your liking.\n\n\n\nTo publish your revisions, we need you to open a pull request. And in order for us to merge your pull request, here’s what we’ll need from you in addition to your content changes.\nBegin with a clean branch (no uncommitted changes). Then run the notebook from the command line:\nmake execute\nThis command will update index.ipynb with the latest execution results.\nThen run make preview to see how the publication is rendering. Verify that your changes appear how you intend them to appear. If not, make the necessary changes and re-run make execute.\nOnce everything looks good, commit index.ipynb and all files in the _freeze/ directory.\nFinally, submit a pull request and we’ll work with you to merge your changes.\nOnce we approve and merge your pull request, we’ll publish a new version of the pub. We’ll notify you when this new version goes live at the hosted URL. Thanks for contributing!"
  },
  {
    "objectID": "pages/CONTRIBUTING.html#getting-started",
    "href": "pages/CONTRIBUTING.html#getting-started",
    "title": "Contributing",
    "section": "",
    "text": "If you haven’t already, follow our setup guide to create a local copy of the code and compute environment."
  },
  {
    "objectID": "pages/CONTRIBUTING.html#make-your-changes",
    "href": "pages/CONTRIBUTING.html#make-your-changes",
    "title": "Contributing",
    "section": "",
    "text": "Edit index.ipynb to your liking."
  },
  {
    "objectID": "pages/CONTRIBUTING.html#steps-before-publishing",
    "href": "pages/CONTRIBUTING.html#steps-before-publishing",
    "title": "Contributing",
    "section": "",
    "text": "To publish your revisions, we need you to open a pull request. And in order for us to merge your pull request, here’s what we’ll need from you in addition to your content changes.\nBegin with a clean branch (no uncommitted changes). Then run the notebook from the command line:\nmake execute\nThis command will update index.ipynb with the latest execution results.\nThen run make preview to see how the publication is rendering. Verify that your changes appear how you intend them to appear. If not, make the necessary changes and re-run make execute.\nOnce everything looks good, commit index.ipynb and all files in the _freeze/ directory.\nFinally, submit a pull request and we’ll work with you to merge your changes.\nOnce we approve and merge your pull request, we’ll publish a new version of the pub. We’ll notify you when this new version goes live at the hosted URL. Thanks for contributing!"
  },
  {
    "objectID": "pages/SETUP.html",
    "href": "pages/SETUP.html",
    "title": "Setup",
    "section": "",
    "text": "This document details how to create a local copy of this pub’s codebase, setup your compute environment, and reproduce the pub itself. This will enable you to experiment with the analysis in the pub and, optionally, contribute revisions to it.\n\n\nThe codebase is hosted on GitHub and can be found here.\nTo obtain a local copy of this repo, you can either clone it directly or fork it to your own GitHub account, then clone your fork. If you aren’t sure what’s best, our suggestion is to clone directly unless you both (1) want to propose a revision for the publication and (2) are not an employee of Arcadia Science.\nTo clone:\ngit clone https://github.com/Arcadia-Science/2025-geno-pheno-attention.git\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe publication is rendered with Quarto. If you don’t have it installed (check with quarto --version), you can install it here.\n\n\nThis repository uses conda to manage the computational and build environment. If you don’t have it installed (check with conda --version), you can find operating system-specific instructions for installing miniconda here. After installing, run the following commands to create and activate the environment.\nconda env create -n 2025-geno-pheno-attention --file env.yml\nconda activate 2025-geno-pheno-attention\nNow, install any internal packages in the repository:\npip install -e .\nAnd finally, if you plan to submit a pull request, install the pre-commit hooks:\npre-commit install"
  },
  {
    "objectID": "pages/SETUP.html#obtain-local-copy",
    "href": "pages/SETUP.html#obtain-local-copy",
    "title": "Setup",
    "section": "",
    "text": "The codebase is hosted on GitHub and can be found here.\nTo obtain a local copy of this repo, you can either clone it directly or fork it to your own GitHub account, then clone your fork. If you aren’t sure what’s best, our suggestion is to clone directly unless you both (1) want to propose a revision for the publication and (2) are not an employee of Arcadia Science.\nTo clone:\ngit clone https://github.com/Arcadia-Science/2025-geno-pheno-attention.git"
  },
  {
    "objectID": "pages/SETUP.html#installation",
    "href": "pages/SETUP.html#installation",
    "title": "Setup",
    "section": "",
    "text": "Important\n\n\n\nThe publication is rendered with Quarto. If you don’t have it installed (check with quarto --version), you can install it here.\n\n\nThis repository uses conda to manage the computational and build environment. If you don’t have it installed (check with conda --version), you can find operating system-specific instructions for installing miniconda here. After installing, run the following commands to create and activate the environment.\nconda env create -n 2025-geno-pheno-attention --file env.yml\nconda activate 2025-geno-pheno-attention\nNow, install any internal packages in the repository:\npip install -e .\nAnd finally, if you plan to submit a pull request, install the pre-commit hooks:\npre-commit install"
  },
  {
    "objectID": "pages/SETUP.html#reproduce",
    "href": "pages/SETUP.html#reproduce",
    "title": "Setup",
    "section": "Reproduce",
    "text": "Reproduce\nThe best way to ensure you’ve correctly set up your code and compute environment is to reproduce this work. Fortunately, the analysis, and therefore the publication itself, can be reproduced with the following command:\nmake execute\n(Make sure you’re in the conda environment you created above)\nThis will execute and render the notebook index.ipynb, then build the publication site. To preview the site, use\nmake preview\nThis will open a local instance of the publication in your default browser."
  },
  {
    "objectID": "pages/SETUP.html#modify",
    "href": "pages/SETUP.html#modify",
    "title": "Setup",
    "section": "Modify",
    "text": "Modify\nTo modify or extend any analyses, open up index.ipynb with Jupyter or your favorite IDE. To preview changes as you modify the notebook, run make preview again and leave the command running. As you make changes to the notebook, the preview site will automatically reload."
  },
  {
    "objectID": "pages/SETUP.html#publish",
    "href": "pages/SETUP.html#publish",
    "title": "Setup",
    "section": "Publish",
    "text": "Publish\nIf you’ve improved the publication, consider contributing so we can update the hosted publication with your edits (big or small!). To get started, see our contributing guide."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "",
    "text": "AI usage disclosure\n\n\n\n\n\nWe used Claude to help write code and clean up code. We used ChatGPT to help write code, clean up code, write text that we edited, suggest wording ideas and then chose which small phrases or sentence structure ideas to use, expand on summary text that we provided and then edited the text it produced, help clarify and streamline text that we wrote, interpret model training results data, and suggest papers on relevant science, we did further reading, and we cited some of this literature. We also provided ChatGPT with starting text and had it rearrange that text to fit the structure of one of our pub templates. We used Gemini in similar ways to help write code, clean up code, write text that we edited, suggest wording ideas and then chose which small phrases or sentence structure ideas to use, help clarify and streamline text that we wrote, interpret model training results data, and suggest papers on relevant science, we did further reading, and we cited some of this literature. We also provided Gemini with starting text and had it rearrange that text to fit the structure of one of our pub templates, and expanded on summary text that we provided and then edited the text it produced."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Introduction",
    "text": "Introduction\nOn April 11th, Rijal et al. (2025) released a preprint that introduces an application of attention mechanisms for inferring genotype-phenotype maps, particularly focusing on capturing complex epistatic interactions. This work sparked considerable interest given our ongoing exploration of nonlinear genotype-phenotype models.\nBy training on 100,000 yeast segregants over 18 growth phenotypes, Rijal et al. (2025) demonstrated that an attention-based architecture extracts more epistatic signals than conventional linear methods, and does so with far fewer parameters than the full second-order regression that has become a norm in quantitative genetics. Their study therefore provides an important proof-of-principle for deep-learning architectures being able to cope with linkage disequilibrium, noise, and sparse high-order interactions in large-scale genotype-phenotype data.\nThat success naturally raised our curiosity. The network they used omits several standard transformer components that brought attention into the limelight: skip connections, layer normalisation, and feed-forward sub-layers (Vaswani et al., 2017). Given the cross-domain success that transformers have seen, we wondered: Could the performance be further improved by replacing their model with a standard “vanilla” transformer architecture? We found out. Also, along the way, we uncovered a missed opportunity by Rijal et al. (2025) to reinforce the learning signal by properly leveraging cross-phenotype genetic correlations, which led to significant performance gains.\n\n\n\n\n\n\n Lightning fast shareable research\n\n\n\nWe offer this work not just as a technical add-on but as a proof of how open, fast-moving collaboration can accelerate discovery. Rijal et al. (2025) shared their code and within twenty days of their preprint—and after only nine working days—we were able to validate, extend, and publicly release the next iteration. That cycle is orders of magnitude quicker than the traditional journal pipeline and illustrates the compounding value of doing bio ML in the open. We hope the present study sparks yet another round of improvements."
  },
  {
    "objectID": "index.html#the-dataset",
    "href": "index.html#the-dataset",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "The dataset",
    "text": "The dataset\nThe experimental data used in Rijal et al. (2025) comes from the work of Nguyen Ba et al. (2022), who performed a large-scale quantitative trait locus (QTL) study in yeast. In short, they measured the growth rates of ~100,000 yeast segregants across 18 conditions and for ~40,000 loci, creating a massive dataset suitable for mapping genotype to phenotype.\nDue to extensive linkage disequilibrium (LD), the loci in the dataset are highly correlated with each other. To create a set of independent loci, Rijal et al. (2025) defined a set of loci such that the correlation between the SNPs present at any pair of loci is less than 94%, resulting in a set of 1164 “independent” loci.\nWe were unable to find the set of loci, nor the genotypic and phenotypic data used for training, so we located the raw data that Nguyen Ba et al. (2022) originally uploaded alongside their study, then used this notebook uploaded by Rijal et al. (2025) to recapitulate the 1164 loci. To save everyone else the trouble, we uploaded the train, test, and validation datasets we’re pretty sure Rijal et al. (2025) used in their study.\nYou can find the data here: https://zenodo.org/records/15313069\nWe’ll use this data in what follows, so let’s go ahead and download it into the current working directory:\n\nimport subprocess\nfrom pathlib import Path\n\ndataset_dir = Path(\"datasets/\")\nif not dataset_dir.exists():\n    zenodo_url = \"https://zenodo.org/records/15313069/files/datasets.zip\"\n    subprocess.run([\"wget\", zenodo_url])\n    subprocess.run([\"unzip\", \"datasets.zip\"])\n    Path(\"datasets.zip\").unlink()\n\n\n\n\n\n\nTable 1. The test dataset R2 values for the\nattention model in Figure 3 of Rijal et al. Calculated by manual pixel\ncounting. {#8d4b415c}\n  \n    \n      \n      Phenotype\n      Source R²\n    \n  \n  \n    \n      0\n      23C\n      0.612\n    \n    \n      1\n      25C\n      0.621\n    \n    \n      2\n      27C\n      0.632\n    \n    \n      3\n      30C\n      0.609\n    \n    \n      4\n      33C\n      0.615\n    \n    \n      5\n      35C\n      0.582\n    \n    \n      6\n      37C\n      0.566\n    \n    \n      7\n      cu\n      0.610\n    \n    \n      8\n      suloc\n      0.638\n    \n    \n      9\n      ynb\n      0.487\n    \n    \n      10\n      eth\n      0.604\n    \n    \n      11\n      gu\n      0.566\n    \n    \n      12\n      li\n      0.630\n    \n    \n      13\n      mann\n      0.600\n    \n    \n      14\n      mol\n      0.618\n    \n    \n      15\n      raff\n      0.653\n    \n    \n      16\n      sds\n      0.630\n    \n    \n      17\n      4NQO\n      0.609"
  },
  {
    "objectID": "index.html#reproducing-single-phenotype-results",
    "href": "index.html#reproducing-single-phenotype-results",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Reproducing single phenotype results",
    "text": "Reproducing single phenotype results\nLet’s first try and reproduce the single-phenotype attention model performances observed in Figure 3 (red dots). This will give us the confidence to know we both (a) correctly reverse-engineered the specifics of their training/validation/test datasets and (b) accurately implemented their model.\n\n\n\nFigure 3 from Rijal et al. (2025). Original caption: “Comparison of model performance in yeast QTL mapping data. We show R² on test datasets for linear, linear+pairwise, and attention-based model (with d = 12) across 18 phenotypes (relative growth rates in various environments). For the linear + pairwise mode, the causal loci inferred by Nguyen Ba et al. (2022) are used.”\n\n\nNote that a separate model is trained on each phenotype, so reproducing this figure involves training 18 models. To do this, we need to create config objects specifying each model architecture and how the training should proceed.\n\n\nCode\nimport attrs\nfrom analysis.base import ModelConfig, TrainConfig\nfrom analysis.dataset import phenotype_names\n\nmodel_config = ModelConfig(\n    model_type=\"rijal_et_al\",\n    seq_length=1164,\n    embedding_dim=13,\n    num_layers=3,\n)\n\n# This template config sets all the shared parameters.\ntrain_template_config = TrainConfig(\n    data_dir=dataset_dir,\n    save_dir=Path(\"models/fig3\"),\n    name_prefix=\"fig3_23C\",\n    phenotypes=[\"23C\"],\n    optimizer=\"adam\",\n    batch_size=64,\n    learning_rate=0.001,\n    lr_schedule=False,\n    weight_decay=0.0,\n    max_epochs=200,\n    gradient_clip_val=0,\n    use_cache=True,\n    use_modal=False,\n)\n\njobs = []\nfor phenotype in phenotype_names:\n    # Each train config needs to set its corresponding phenotype(s).\n    phenotype_config = attrs.evolve(\n        train_template_config, phenotypes=[phenotype], name_prefix=f\"fig3_{phenotype}\"\n    )\n    jobs.append((model_config, phenotype_config))\n\n\nWith each job properly configured, we can train a model for each phenotype:\n\n\nCode\nfrom analysis.train import run_trainings\n\nmodel_dirs = run_trainings(jobs)\n\n\nPre-trained model 'fig3_23C' found. Returning path.\nPre-trained model 'fig3_25C' found. Returning path.\nPre-trained model 'fig3_27C' found. Returning path.\nPre-trained model 'fig3_30C' found. Returning path.\nPre-trained model 'fig3_33C' found. Returning path.\nPre-trained model 'fig3_35C' found. Returning path.\nPre-trained model 'fig3_37C' found. Returning path.\nPre-trained model 'fig3_cu' found. Returning path.\nPre-trained model 'fig3_suloc' found. Returning path.\nPre-trained model 'fig3_ynb' found. Returning path.\nPre-trained model 'fig3_eth' found. Returning path.\nPre-trained model 'fig3_gu' found. Returning path.\nPre-trained model 'fig3_li' found. Returning path.\nPre-trained model 'fig3_mann' found. Returning path.\nPre-trained model 'fig3_mol' found. Returning path.\nPre-trained model 'fig3_raff' found. Returning path.\nPre-trained model 'fig3_sds' found. Returning path.\nPre-trained model 'fig3_4NQO' found. Returning path.\n\n\n\n\n\n\n\n\nA note about training behavior\n\n\n\nThe above code will initiate training for all configured jobs, with important considerations:\n\nCaching: Since train_config.use_cache = True and these models are stored in the GitHub repository, executing this locally will use cached models instead of performing expensive retraining\nTraining Duration: Each training job takes approximately 2.5 hours to complete on an A10G GPU, so running all jobs without caching would require significant time\nCompute Configuration: In our production environment, we set train_config.use_modal = True to distribute compute jobs via Modal. For compatibility with different compute architectures, this notebook uses train_config.use_modal = False by default.\n\n\n\nInside each run directory is a metrics.csv that we can get the test dataset \\(R^2\\) from and compare directly against Figure 3 from Rijal et al. (2025).\n\n\nCode\nfrom pathlib import Path\n\nimport arcadia_pycolor as apc\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\napc.mpl.setup()\n\n\ndef get_test_r2(model_dir: Path) -&gt; float:\n    metrics = pd.read_csv(model_dir / \"metrics.csv\")\n    return float(metrics.loc[metrics[\"metric\"] == \"test_r2\", \"value\"].iloc[0])\n\n\nfig3_reproduction_r2 = [get_test_r2(d) for d in model_dirs]\nfig3_results[\"reproduction R2\"] = fig3_reproduction_r2\ndf = pd.DataFrame(fig3_results)\n\nx = np.arange(len(df))\nwidth = 0.35\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    plt.bar(x - width / 2, df[\"source R2\"], width, label=\"Source R²\", color=apc.cloud)\n    plt.bar(x + width / 2, df[\"reproduction R2\"], width, label=\"Reproduction R²\", color=apc.steel)\n\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"R² Score\")\n    plt.ylim(0.46, 0.67)\n    plt.xticks(x, df[\"phenotype\"], rotation=90)\n    plt.legend(loc=(0.05, 0.90), ncol=2)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\nFigure 1: Comparison of test-set \\(R^2\\) values between Rijal et al. (2025) (“Source”) and our re-implementation (“Reproduction”).\n\n\n\n\n\nWith an \\(n\\) of 1, we can’t assess reproducibility rigorously. Even so, our re-implementation matches the published numbers very closely. Here are some numbers on the phenotype with the largest deviation:\n\n\nCode\ndeltas = (df[\"source R2\"] - df[\"reproduction R2\"]).abs()\nmax_idx = deltas.argmax()\nmax_delta = deltas.max()\nphenotype_with_largest_delta = df[\"phenotype\"].iloc[max_idx]\npercent_diff = 100 * max_delta / df[\"source R2\"].iloc[max_idx]\n\nprint(\"Biggest discrepenacy:\")\nprint(f\"- Phenotype: {phenotype_with_largest_delta}\")\nprint(f\"- R2 difference: {max_delta:.3f}\")\nprint(f\"- Percent difference: {percent_diff:.1f}\")\n\n\nBiggest discrepenacy:\n- Phenotype: 33C\n- R2 difference: 0.008\n- Percent difference: 1.3\n\n\nOverall, these results help assure us that:\n\nThe train/validation/test partitions are identical, or at least functionally equivalent\nOur code reproduces the authors’ architecture and training procedure.\n\nWith this validated baseline in place, we’re now confident in using it as the starting point for modifying the architecture."
  },
  {
    "objectID": "index.html#canonical-ml-components-outperform-bespoke-customization",
    "href": "index.html#canonical-ml-components-outperform-bespoke-customization",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Canonical ML components outperform bespoke customization",
    "text": "Canonical ML components outperform bespoke customization\nThe Rijal et al. (2025) model architecture uses non-standard components. To test whether these idiosyncratic choices are actually helpful, we replaced each one with a standard, “textbook” alternative and measured the collective impact on predictive accuracy.\n\n\n\n\n\n\n\n\nNon-standard element in Rijal et al.\nWhat it does\nCanonical replacement we tried\n\n\n\n\nRandom projection of a diagonal genotype matrix X(g)·R\nEncodes each locus as a row of a fixed random matrix R, multiplied by the allele sign (±1).\nA learned embedding table nn.Embedding(L, D) whose rows are multiplied by the allele sign.\n\n\nConcatenated column of ones\nAppends a constant 1 to every token to mimic an explicit bias term (Appendix F in the original paper).\nSimply enable the bias in the linear Q, K, V projections (nn.Linear(..., bias=True)).\n\n\nPhenotype represented as an input token\nAdds one-hot phenotype tokens to the sequence, forcing attention layers to discover gene × phenotype interactions.\nCondition the model after the attention block: predict all phenotypes jointly from a pooled sequence representation (see below).\n\n\nFlattened fitness matrix\nTreats each (genotype, phenotype) pair as an independent sample; the network outputs one scalar at a time.\nMean-pool the token embeddings → (B, D) and use a single linear layer D → 18 so all phenotypes are predicted simultaneously.\n\n\n\n\nWhy predict all phenotypes at once?\nThe measured phenotypes are likely to be genetically correlated due to biological processes such as pleiotropy. Consequently, knowing that a mutation hurts growth in one condition has the potential to inform its effect in another. A shared output head lets the network exploit this mutual information, whereas the original set-up can only share knowledge through the shared attention weights. Our phenotype-phenotype autoencoder study (Avasthi et al., 2023) showed significant prediction benefits when accounting for phenotypic covariation, so we’re expecting the same thing to be true here, too.\n\n\nExperimental protocol\nThe codebase contains a new architecture with the above modifications. Let’s test its performance with the following changes to the training:\n\nIncrease the number of phenotypes from \\(1\\) to \\(18\\) (all).\nCorrespondingly increase the hidden dimension from \\(d=12\\) to \\(d=128\\).\nDecrease the learning rate 10-fold, from \\(1 \\times 10^{-3}\\) to \\(1 \\times 10^{-4}\\).\n\nFinally, because Figure 1 hints at potentially significant run-to-run variance, let’s run five replicates, where each replicate differs only in its initializing seed (the train/validation/test splits are held constant).\n\n\nCode\nNUM_REPLICATES = 5\n\njobs = []\nfor i in range(NUM_REPLICATES):\n    replicate_id = f\"{i:02d}\"\n    job_name_prefix = f\"std_d128_rep_{replicate_id}\"\n\n    model_config = ModelConfig(\n        embedding_dim=128,\n        model_type=\"modified\",\n        seq_length=1164,\n        num_layers=3,\n    )\n\n    train_config = attrs.evolve(\n        train_template_config,\n        save_dir=Path(\"models/canonical\"),\n        phenotypes=phenotype_names,\n        name_prefix=job_name_prefix,\n        optimizer=\"adamw\",\n        max_epochs=100,\n    )\n\n    jobs.append((model_config, train_config))\n\nprint(f\"\\nGenerated {len(jobs)} job configurations.\")\n\n\n\nGenerated 5 job configurations.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re running this yourself and aren’t interested in replicates, you can reduce the amount of required compute by setting NUM_REPLICATES in the above cell to 1.\n\n\nNow, let’s run the experiment:\n\n\nCode\nmodel_dirs = run_trainings(jobs)\n\n\nPre-trained model 'std_d128_rep_00' found. Returning path.\nPre-trained model 'std_d128_rep_01' found. Returning path.\nPre-trained model 'std_d128_rep_02' found. Returning path.\nPre-trained model 'std_d128_rep_03' found. Returning path.\nPre-trained model 'std_d128_rep_04' found. Returning path."
  },
  {
    "objectID": "index.html#consistent-gains-across-phenotypes",
    "href": "index.html#consistent-gains-across-phenotypes",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Consistent gains across phenotypes",
    "text": "Consistent gains across phenotypes\nFigure 2 shows that swapping the custom choices in Rijal et al. (2025) for a canonical embedding \\(\\rightarrow\\) mean-pool \\(\\rightarrow\\) linear-head model and predicting all 18 phenotypes jointly yields a boost in predictive power across the board:\n\n\nCode\nlabel_lookup = {\n    \"std\": \"Canonical\",\n}\n\n\ndef get_phenotype_r2_data(model_dir: Path) -&gt; float:\n    # Determine the architecture and replicate number from model directory name\n    dir_name = model_dir.parent.parent.name  # e.g., \"std_d128_rep_09\"\n    variant_key, _, rep_str = dir_name.rpartition(\"_rep_\")\n    base_arch_key, _, _ = variant_key.rpartition(\"_d\")\n    architecture_label = label_lookup[base_arch_key]\n\n    # Load and wrangle the metrics.csv. Keep only the per-phenotype test R2 values.\n    df = pd.read_csv(model_dir / \"metrics.csv\")\n    df = df[df[\"metric\"].str.startswith(\"test_r2_\")]\n    df[\"phenotype\"] = df[\"metric\"].str[8:]\n    df.drop(\"metric\", axis=1, inplace=True)\n    df.rename(columns={\"value\": \"r2\"}, inplace=True)\n    df[\"architecture\"] = architecture_label\n    df[\"replicate\"] = int(rep_str)\n    df[\"r2\"] = df[\"r2\"].astype(float)\n\n    return df\n\n\n# Concat the phenotype data across all models\ncanonical_plot_data = pd.concat([get_phenotype_r2_data(model_dir) for model_dir in model_dirs])\n\n# Calculate the mean and standard error R2 over replicates\ncanonical_plot_data = (\n    canonical_plot_data.groupby([\"architecture\", \"phenotype\"])[\"r2\"]\n    .agg(mean_r2=\"mean\", std_r2=\"std\")\n    .reset_index()\n)\n\n# Concatenate the manually scraped Fig3 R2s\nrijal_r2s = fig3_results.copy(deep=True)\nrijal_r2s.drop(\"reproduction R2\", axis=1, inplace=True)\nrijal_r2s.rename(columns={\"source R2\": \"mean_r2\"}, inplace=True)\nrijal_r2s[\"std_r2\"] = 0.0\nrijal_r2s[\"architecture\"] = \"Rijal\"\ncanonical_plot_data = pd.concat([canonical_plot_data, rijal_r2s])\n\n# Plotting code.\ncanonical_plot_data[\"phenotype\"] = pd.Categorical(\n    canonical_plot_data[\"phenotype\"], categories=phenotype_names, ordered=True\n)\narch_order = [\"Rijal\", \"Canonical\"]\n\nx = np.arange(len(phenotype_names))\nwidth = 0.25\n\ncolor_map = {\n    \"Rijal\": apc.cloud,\n    \"Canonical\": apc.mars,\n}\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    for i, arch in enumerate(arch_order):\n        sub = (\n            canonical_plot_data[canonical_plot_data[\"architecture\"] == arch]\n            .set_index(\"phenotype\")\n            .reindex(phenotype_names)\n        )\n        plt.bar(\n            x + (i - 0.5) * width,  # centre bars around tick\n            sub[\"mean_r2\"],\n            width,\n            yerr=sub[\"std_r2\"],\n            ecolor=\"#00000099\",\n            error_kw=dict(elinewidth=1.5),\n            label=arch,\n            color=color_map[arch],\n        )\n\n    plt.xticks(x, phenotype_names, rotation=90)\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"Mean $R^2$ (± SD)\")\n    plt.ylim(0.43, 0.69)\n    plt.legend(loc=(0.05, 0.92), ncol=3)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\nFigure 2: Per-phenotype generalisation of multi-task vs. single-task models. Bar heights show the mean test-set \\(R^{2}\\) achieved across replicates for each phenotype, with error bars denoting the standard deviation (SD). “Rijal” is the single-phenotype results reported in Rijal et al. (2025), while “Canonical” is our multi-output head trained to predict all phenotypes jointly.\n\n\n\n\n\n\n\n\n\n\n\nImportant caveat on the baseline we compare against\n\n\n\nThe numbers labeled “Rijal” in Figure 2 come from the single-phenotype models reported in their Figure 3. Rijal et al. (2025) do develop a multi-phenotype architecture, but careful inspection of their code shows that it still emits a scalar output per forward pass and simply loops over phenotypes at train time. In other words, each (genotype, phenotype) pair is treated as an independent sample. The predictive power of this model is shown in their Figure 4, and is actually worse than the single-phenotype models at predicting any given phenotype.\nBecause our goal is to test whether a genuine multi-output head + canonical components confer an advantage, we chose the best-performing baseline available (their single-phenotype models). The comparison is therefore conservative: any gains we report would be larger, not smaller, if the authors’ scalar multi-phenotype model were used as the reference.\n\n\nQuantifying which changes contributed to these gains would require an ablation study, however our hunch is that the primary gains come from cross-phenotype genetic correlations. For instance we note that some of the most consistent gains we see in our implementation of the model come from fitness measurements at different temperatures, a set of phenotypes that is almost certainly impacted by pleiotropic sets of genes. Joint training allows the network to transfer information among genetically correlated traits, an advantage that the single-output baseline can’t exploit beyond implicit relationships learned through shared attention weights.\nTaken together, these results validate the canonical multi-output model as a stronger starting point than the original design."
  },
  {
    "objectID": "index.html#a-vanilla-transformer-architecture",
    "href": "index.html#a-vanilla-transformer-architecture",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "A vanilla transformer architecture",
    "text": "A vanilla transformer architecture\nThe current architecture still departs from the reference transformer in several respects: it omits residual connections, layer normalization, and position-wise feed-forward blocks. The logical next experiment is therefore to level up to a bona-fide vanilla transformer, preserving the current tokenization while adding:\n\nResidual (skip) connections and pre-LayerNorm,\n\nFeed-forward sub-layers with RELU activations,\n\nScaled dot-product attention with the canonical \\(1/\\sqrt{d}\\) factor,\n\nDropout and weight-decay for regularization to prevent overfitting.\n\nIt’s worth noting that transformers excel at sequence modeling—and sequences are ordered collections. While the loci in this dataset could be attributed chromosomal coordinates, and therefore in a sense an ordering, like Rijal et al. (2025), we’re treating the loci as an unordered collection. Thus, we don’t add any positional encodings, either in the form of absolute or relative positional encodings.\n\n\nCode\nNUM_REPLICATES = 5\n\njobs = []\nfor i in range(NUM_REPLICATES):\n    replicate_id = f\"{i:02d}\"\n    job_name_prefix = f\"xformer_rep_{replicate_id}\"\n\n    model_config = ModelConfig(\n        embedding_dim=256,\n        model_type=\"transformer\",\n        seq_length=1164,\n        num_layers=3,\n        nhead=4,\n        dim_feedforward=1024,\n    )\n\n    train_config = attrs.evolve(\n        train_template_config,\n        save_dir=Path(\"models/transformer\"),\n        phenotypes=phenotype_names,\n        name_prefix=job_name_prefix,\n        optimizer=\"adamw\",\n        max_epochs=80,\n    )\n\n    jobs.append((model_config, train_config))\n\nprint(f\"\\nGenerated {len(jobs)} job configurations.\")\n\n\n\nGenerated 5 job configurations.\n\n\n\n\nCode\nmodel_dirs = run_trainings(jobs)\n\n\nPre-trained model 'xformer_rep_00' found. Returning path.\nPre-trained model 'xformer_rep_01' found. Returning path.\nPre-trained model 'xformer_rep_02' found. Returning path.\nPre-trained model 'xformer_rep_03' found. Returning path.\nPre-trained model 'xformer_rep_04' found. Returning path.\n\n\nFigure 3 shows the change in performance:\n\n\nCode\nlabel_lookup = {\n    \"std\": \"Canonical\",\n    \"xformer\": \"Transformer\",\n}\n\n\ndef get_phenotype_r2_data_xformer(model_dir: Path) -&gt; float:\n    # Determine the architecture and replicate number from model directory name\n    dir_name = model_dir.parent.parent.name  # e.g., \"std_d128_rep_09\"\n    variant_key, _, rep_str = dir_name.rpartition(\"_rep_\")\n    architecture_label = label_lookup[variant_key]\n\n    # Load and wrangle the metrics.csv. Keep only the per-phenotype test R2 values.\n    df = pd.read_csv(model_dir / \"metrics.csv\")\n    df = df[df[\"metric\"].str.startswith(\"test_r2_\")]\n    df[\"phenotype\"] = df[\"metric\"].str[8:]\n    df.drop(\"metric\", axis=1, inplace=True)\n    df.rename(columns={\"value\": \"r2\"}, inplace=True)\n    df[\"architecture\"] = architecture_label\n    df[\"replicate\"] = int(rep_str)\n    df[\"r2\"] = df[\"r2\"].astype(float)\n\n    return df\n\n\n# Concat the phenotype data across all models\ndf = pd.concat([get_phenotype_r2_data_xformer(model_dir) for model_dir in model_dirs])\ndf = df.groupby([\"architecture\", \"phenotype\"])[\"r2\"].agg(mean_r2=\"mean\", std_r2=\"std\").reset_index()\n\n# Concat with existing canonical plot data\nxformer_plot_data = pd.concat([df, canonical_plot_data])\n\n# Plotting code.\nxformer_plot_data[\"phenotype\"] = pd.Categorical(\n    xformer_plot_data[\"phenotype\"], categories=phenotype_names, ordered=True\n)\narch_order = [\"Rijal\", \"Canonical\", \"Transformer\"]\n\nx = np.arange(len(phenotype_names))\nwidth = 0.25\n\ncolor_map = {\n    \"Rijal\": apc.cloud,\n    \"Canonical\": apc.mars,\n    \"Transformer\": apc.dragon,\n}\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    for i, arch in enumerate(arch_order):\n        sub = (\n            xformer_plot_data[xformer_plot_data[\"architecture\"] == arch]\n            .set_index(\"phenotype\")\n            .reindex(phenotype_names)\n        )\n        plt.bar(\n            x + (i - 1) * width,  # centre bars around tick\n            sub[\"mean_r2\"],\n            width,\n            yerr=sub[\"std_r2\"],\n            ecolor=\"#00000099\",\n            error_kw=dict(elinewidth=1.5),\n            label=arch,\n            color=color_map[arch],\n        )\n\n    plt.xticks(x, phenotype_names, rotation=90)\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"Mean $R^2$ (± SD)\")\n    plt.ylim(0.43, 0.69)\n    plt.legend(loc=(0.05, 0.92), ncol=3)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\nFigure 3: The vanilla transformer performances added alongside the models compared in Figure 2.\n\n\n\n\n\nThe full transformer nudges performance above the already-improved canonical model across most phenotypes (Figure 3). The lift is apparent, but no stepwise performance gains are observed. There’s undoubtedly a lot of room for improvement: we made no attempt at hyper-parameter tuning beyond some cursory investigations of dropout and weight-decay (neither of which positively affected performance), and all other parameters were guessed zero shot."
  },
  {
    "objectID": "index.html#conclusion-and-outlook",
    "href": "index.html#conclusion-and-outlook",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Conclusion and outlook",
    "text": "Conclusion and outlook\nOur central question was simple: do off-the-shelf transformer components—and a genuine multi-output head—move the needle on genotype to phenotype prediction?\nThe answer is an emphatic yes.\n\nWhy multi-output matters\nLeveraging mutual information between genetically correlated phenotypes represents a natural way of boosting model performance when training data is limited. In the context of plant/animal breeding, multi-trait genomic prediction has been well established to improve the predictive power of linear models (particularly for low heritability traits) (Jia and Jannink, 2012). Our own work with phenotype-phenotype autoencoders has demonstrated that encoding multiple phenotypes jointly lets an auto-encoder predict individual phenotypes with very high accuracy, particularly as the number of phenotypes considered simultaneously increases (Avasthi et al., 2023). These observations are examples of the benefits of multi-task learning, which have long been appreciated in ML literature (Caruana, 1997). Our presented results are no different: when the network can see all 18 phenotypes at once, shared genetic effects (either through pleiotropy or in some instances through LD (Chebib and Guillaume, 2021)) reinforce, rather than fragment, the learning signal.\n\n\nA foundation, not a finish line\nA natural next step given these results is to test cross-environment transfer learning in our multi-task model. Rijal et al. (2025) demonstrated that transfer learning was possible across temperature growth conditions even with a multi-environment model that did not outperform a single-environment model. Given the overall superior performance of the models we present here, they may be particularly well suited for fitness prediction for novel phenotypes with even fewer fine-tuning observations than used by Rijal et al. (2025).\nFinally, it would be interesting to evaluate where the performance boost of multi-task learning comes from, from a quantitative genetics perspective. Phenotypic variance can generally be broken down into additive (\\(G\\)), epistatic (\\(G \\times G\\)) and gene by environment (\\(G \\times E\\)) components (ignoring higher order terms for simplicity). Intuitively, we might expect multi-trait learning to excel at explaining \\(G\\) and \\(G \\times G\\) variance components that are constant across environments, however it’s possible that \\(G \\times E\\) variance is also better explained, as multi-task learning benefits are known to extend to even unrelated tasks (Paredes et al., 2012). We suspect this could be investigated through ablation studies that track environment specific prediction outcomes.\nThere is also still plenty of headroom for improving the model:\n\nNo hyper-parameter search was attempted for the transformer; depth, head count, LR schedules, dropout rates, and layer widths all remain untouched.\nHow far can the canonical model be pushed? Maybe the transformer is overkill.\nThe model treats the loci as an unordered collection. It’s possible that adding chromosomal coordinates or some kind of other positional encoding could let the model sing.\nWe haven’t experimented with more sophisticated feature selection methods for reducing the number of loci prior to model training, a task that may be particularly fruitful for improving the ability of models to capture pairwise and higher order epistasis.\n\n\n\nAn invitation to build further\nAll code, configs, and cached model checkpoints are available in this notebook’s repository. The Appendix documents the engineering decisions that should help orient you to the codebase."
  },
  {
    "objectID": "index.html#appendix-a-codebase-primer",
    "href": "index.html#appendix-a-codebase-primer",
    "title": "Cross-trait learning with a canonical transformer tops custom attention in genotype-phenotype mapping –",
    "section": "Appendix: a codebase primer",
    "text": "Appendix: a codebase primer\n\n\n\n\n\n\nNote\n\n\n\nThis section elaborates on engineering details that will be of interest to those planning to reproduce, modify, or build off of this research.\n\n\nRather than building off the Rijal et al. (2025) notebook files, we re-implemented their code into our own codebase to improve code quality and make room for our modifications and experimentation. Here is a very high-level summary of the changes we made:\n\nAdded configuration dataclasses to co-localize tunables\nSaved the training/validation/test datasets to file to avoid splitting on-the-fly\nCreated PyTorch dataloaders to manage accession, batching, and data shuffling\nAutomated the training loop with PyTorch Lightning, creating separation between the training loop and the model logic\nAdded canonical learning parameters like early stopping, learning rate scheduling, gradient norm clipping, weight decay, and more\nGeneralized the Rijal et al. (2025) model with toggleable skip connections, layer normalization, scaled attention, dropout rate, and more\n\nThe upshot is that we’re proud of this code and think it establishes a much-needed foundation that can be used to build off the research seeded by Rijal et al. (2025).\n\nTraining models\nIn the analysis above, we illustrated how multiple training jobs can be run using the high level entrypoint, run_trainings, that:\n\nTrains a model for a given set of phenotypes\nDetermines the best model, defined as the model with the lowest loss (MSE) calculated over the validation dataset\nReports the \\(R^2\\) for the test dataset using the best model\nSaves the model to file for downstream use\n\nThe codebase also exposes equivalent behavior through a command-line interface (CLI). It can be accessed via:\n$ python -m analysis.train --help\n\n\nDistributed computing\nWe performed the analysis using Modal’s cloud infrastructure to distribute computations across GPUs, allowing us to rapidly measure performance across many different model architectures and training specifications. Whether you want to train with or without Modal can be toggled by the attribute train.use_modal. By default, Modal execution is disabled. The downside is that your training jobs will run in serial, rather than being distributed across different machines.\n\n\nCaching behavior\nWe implemented a simple cache mechanism that avoids training if a model directory for a given training config already exists. We did this so that GPUs aren’t a requirement for engaging with this research.\n\ntrain_config.use_cache = True (default): Skips retraining if a model with the same configuration already exists\ntrain_config.use_cache = False: Forces retraining regardless of existing models\n\n\n\n\n\n\n\nNote\n\n\n\nAll training runs in this analysis use the default caching mode (train_config.use_cache = True), and the results are git tracked. As a result, if you execute this notebook locally, these models will be loaded from cache rather than retrained."
  }
]